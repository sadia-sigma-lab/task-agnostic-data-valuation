{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6VbfaM0yTvEnlwmBCviRd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadia-sigma-lab/task-agnostic-data-valuation/blob/main/data_valuation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "def preprocess_images(images, epsilon=1e-11):\n",
        "    images = images.reshape(len(images), -1)  # Flatten the images\n",
        "    images = images.astype('float32') / 255.0  # Normalize pixel values to [0,1]\n",
        "    mean = np.mean(images, axis=0)\n",
        "    std_dev = np.std(images, axis=0)\n",
        "\n",
        "    # Avoid division by zero by adding a small epsilon to std_dev\n",
        "    images = (images - mean) / (std_dev + epsilon)  # Zero-center and normalize\n",
        "    return images\n",
        "\n",
        "# Function to get data for buyer or sellers\n",
        "def get_data(images, labels, classes, exclude_indices=set(), num_samples=10000):\n",
        "    indices = np.where(np.isin(labels, classes))[0]\n",
        "    # Exclude already used indices to ensure distinct images\n",
        "    indices = np.setdiff1d(indices, list(exclude_indices))\n",
        "    np.random.shuffle(indices)\n",
        "    selected_indices = indices[:num_samples]\n",
        "    selected_images = images[selected_indices]\n",
        "    selected_labels = labels[selected_indices]\n",
        "    return selected_images, selected_labels, set(selected_indices)\n",
        "\n",
        "# Function to compute projected eigenvalues\n",
        "def compute_projected_eigenvalues(data, components, num_components):\n",
        "    projected_data = np.dot(data, components.T)\n",
        "    covariance_matrix = np.cov(projected_data, rowvar=True)\n",
        "    eigenvalues = np.linalg.eigvalsh(covariance_matrix)\n",
        "    eigenvalues = np.flip(np.sort(eigenvalues))[:num_components]\n",
        "    return eigenvalues\n",
        "\n",
        "\n",
        "# Function to compute diversity and relevance\n",
        "def compute_diversity_relevance(buyer_eigenvalues, seller_eigenvalues):\n",
        "    max_vals = np.maximum(buyer_eigenvalues, seller_eigenvalues)\n",
        "    min_vals = np.minimum(buyer_eigenvalues, seller_eigenvalues)\n",
        "    abs_diff = np.abs(buyer_eigenvalues - seller_eigenvalues)\n",
        "\n",
        "    # Handle division by zero\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        diversity_components = abs_diff / max_vals\n",
        "        relevance_components = min_vals / max_vals\n",
        "\n",
        "    # Replace NaN values with zero\n",
        "    valid_mask = ~np.isnan(diversity_components) & ~np.isnan(relevance_components)\n",
        "\n",
        "# Apply the mask to filter out rows containing NaN in either array\n",
        "    diversity_components = diversity_components[valid_mask]\n",
        "    relevance_components = relevance_components[valid_mask]\n",
        "    if np.any(diversity_components == 0):\n",
        "      print(\"Zero values found in diversity_components at indices:\", np.where(diversity_components == 0))\n",
        "\n",
        "    if np.any(relevance_components == 0):\n",
        "      print(\"Zero values found in relevance_components at indices:\", np.where(relevance_components == 0))\n",
        "    # diversity_components = np.nan_to_num(diversity_components)\n",
        "    # relevance_components = np.nan_to_num(relevance_components)\n",
        "\n",
        "    diversity = np.prod(diversity_components)\n",
        "    print(\"diver prod\" , diversity)\n",
        "    diversity = diversity ** (1 / len(diversity_components))\n",
        "    relevance = np.prod(relevance_components)\n",
        "    print(\"relevancy prod\" , relevance)\n",
        "    relevance =relevance ** (1 / len(relevance_components))\n",
        "    return diversity, relevance\n",
        "\n",
        "\n",
        "# Load fashion-MNIST dataset\n",
        "(fmnist_train_images, fmnist_train_labels), (fmnist_test_images, fmnist_test_labels) = fashion_mnist.load_data()\n",
        "fmnist_images = np.concatenate((fmnist_train_images, fmnist_test_images))\n",
        "fmnist_labels = np.concatenate((fmnist_train_labels, fmnist_test_labels))\n",
        "\n",
        "# Prepare Buyer's Data\n",
        "buyer_classes = [0, 1, 2, 3, 4]\n",
        "buyer_images, buyer_labels, buyer_indices = get_data(\n",
        "    fmnist_images, fmnist_labels, buyer_classes, num_samples=6000)\n",
        "\n",
        "used_indices = buyer_indices.copy()  # Keep track of used indices to ensure distinct datasets\n",
        "\n",
        "# Prepare Sellers' Data\n",
        "sellers_info = [\n",
        "    (\"Seller 1 (Classes 0-4)\", [0, 1, 2, 3, 4]),\n",
        "    (\"Seller 2 (Classes 1-5)\", [1, 2, 3, 4, 5]),\n",
        "    (\"Seller 3 (Classes 0-9)\", list(range(0, 10))),\n",
        "    (\"Seller 4 (Classes 3-9)\", [3, 4, 5, 6, 7, 8, 9]),\n",
        "    (\"Seller 5 (Classes 5-9)\", [5, 6, 7, 8, 9])\n",
        "]\n",
        "\n",
        "seller_images_list = []\n",
        "seller_labels_list = []\n",
        "seller_names = []\n",
        "for name, classes in sellers_info:\n",
        "    images, labels, indices = get_data(\n",
        "        fmnist_images, fmnist_labels, classes, exclude_indices=used_indices, num_samples=10000)\n",
        "        # fmnist_images, fmnist_labels, classes,  num_samples=10000)\n",
        "    used_indices.update(indices)\n",
        "    seller_images_list.append(images)\n",
        "    seller_labels_list.append(labels)\n",
        "    unique_labels = np.unique(labels)\n",
        "    print(\"labels \" , unique_labels)\n",
        "    print(\"seller\" , name)\n",
        "    seller_names.append(name)\n",
        "\n",
        "# Sellers from fashion-MNIST\n",
        "# Seller 6: Sandal (Class 5)\n",
        "seller6_indices = np.where(fmnist_labels == 5)[0]\n",
        "np.random.shuffle(seller6_indices)\n",
        "seller6_images = fmnist_images[seller6_indices[:10000]]\n",
        "seller6_labels = fmnist_labels[seller6_indices[:10000]]\n",
        "seller_images_list.append(seller6_images)\n",
        "seller_labels_list.append(seller6_labels)\n",
        "seller_names.append(\"Seller 6 (Sandal)\")\n",
        "\n",
        "# Seller 7: Coat (Class 4)\n",
        "seller7_indices = np.where(fmnist_labels == 4)[0]\n",
        "np.random.shuffle(seller7_indices)\n",
        "seller7_images = fmnist_images[seller7_indices[:10000]]\n",
        "seller7_labels = fmnist_labels[seller7_indices[:10000]]\n",
        "seller_images_list.append(seller7_images)\n",
        "seller_labels_list.append(seller7_labels)\n",
        "seller_names.append(\"Seller 7 (Coat)\")\n",
        "\n",
        "# Seller 8: Noisy images\n",
        "seller8_images = np.random.normal(0, 1, (1000, 28, 28))\n",
        "seller8_labels = None  # No labels\n",
        "seller_images_list.append(seller8_images)\n",
        "seller_labels_list.append(seller8_labels)\n",
        "seller_names.append(\"Seller 8 (Noise)\")\n",
        "\n",
        "# Preprocess Buyer's Data\n",
        "buyer_data = preprocess_images(buyer_images)\n",
        "\n",
        "# Perform PCA on Buyer's Data\n",
        "pca = PCA()\n",
        "pca.fit(buyer_data)\n",
        "buyer_eigenvalues = pca.explained_variance_\n",
        "buyer_components = pca.components_\n",
        "\n",
        "# Select principal components with eigenvalues > 1e-2\n",
        "print(\"buyer_eigenvalues\" , buyer_eigenvalues)\n",
        "significant_components = buyer_eigenvalues > 1e-2\n",
        "print('significant_components', significant_components)\n",
        "buyer_eigenvalues = buyer_eigenvalues[significant_components]\n",
        "buyer_components = buyer_components[significant_components]\n",
        "\n",
        "# Compute Diversity and Relevance for Each Seller\n",
        "diversity_vals = []\n",
        "relevance_vals = []\n",
        "\n",
        "for idx, seller_images in enumerate(seller_images_list):\n",
        "    # Preprocess Seller's Data\n",
        "    seller_data = preprocess_images(seller_images)\n",
        "    # Compute Seller's Projected Eigenvalues\n",
        "    seller_eigenvalues = compute_projected_eigenvalues(\n",
        "        seller_data, buyer_components, len(buyer_eigenvalues))\n",
        "    # Compute Diversity and Relevance\n",
        "    diversity, relevance = compute_diversity_relevance(buyer_eigenvalues, seller_eigenvalues)\n",
        "    diversity_vals.append(diversity)\n",
        "    relevance_vals.append(relevance)\n",
        "\n",
        "# Plot Diversity vs. Relevance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(relevance_vals, diversity_vals, color='blue', s=100)\n",
        "for i, txt in enumerate(seller_names):\n",
        "    print(\"seller name\" , txt )\n",
        "    print(\"relevance_vals[i]\" , relevance_vals[i] )\n",
        "    print(\"diversity_vals[i\" , diversity_vals[i] )\n",
        "    plt.annotate(txt, (relevance_vals[i], diversity_vals[i]), fontsize=12)\n",
        "plt.title(\"Diversity vs. Relevance of Seller Data\", fontsize=16)\n",
        "plt.xlabel(\"Relevance\", fontsize=14)\n",
        "plt.ylabel(\"Diversity\", fontsize=14)\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efCL1uTctf0b",
        "outputId": "b71c1530-cc3d-4cc7-d8ba-2c17d203c087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels  [0 1 2 3 4]\n",
            "seller Seller 1 (Classes 0-4)\n",
            "labels  [1 2 3 4 5]\n",
            "seller Seller 2 (Classes 1-5)\n",
            "labels  [0 1 2 3 4 5 6 7 8 9]\n",
            "seller Seller 3 (Classes 0-9)\n",
            "labels  [3 4 5 6 7 8 9]\n",
            "seller Seller 4 (Classes 3-9)\n",
            "labels  [5 6 7 8 9]\n",
            "seller Seller 5 (Classes 5-9)\n",
            "buyer_eigenvalues [1.76943192e+02 9.91036224e+01 5.60928421e+01 4.15209656e+01\n",
            " 2.50868683e+01 2.21514606e+01 1.70868587e+01 1.34529266e+01\n",
            " 1.20916023e+01 1.05052433e+01 9.98393059e+00 8.60867023e+00\n",
            " 8.03807926e+00 7.61218452e+00 7.33735561e+00 6.92736483e+00\n",
            " 6.51128817e+00 6.35023880e+00 5.79067755e+00 5.70379639e+00\n",
            " 5.43475342e+00 5.32658625e+00 4.85942602e+00 4.67801332e+00\n",
            " 4.49129105e+00 4.37573051e+00 4.22644997e+00 3.90164137e+00\n",
            " 3.82400846e+00 3.68091488e+00 3.53925920e+00 3.29139566e+00\n",
            " 3.18084383e+00 3.11155939e+00 2.97305250e+00 2.85405779e+00\n",
            " 2.75256133e+00 2.75095272e+00 2.73274803e+00 2.59841657e+00\n",
            " 2.53749347e+00 2.48084879e+00 2.38457704e+00 2.25743103e+00\n",
            " 2.22309899e+00 2.20116448e+00 2.15300560e+00 2.09868622e+00\n",
            " 2.03157258e+00 1.95568478e+00 1.92539167e+00 1.90051413e+00\n",
            " 1.85191977e+00 1.77885818e+00 1.76296711e+00 1.67060101e+00\n",
            " 1.60540807e+00 1.59344757e+00 1.56849885e+00 1.51666272e+00\n",
            " 1.47210658e+00 1.46726882e+00 1.44570696e+00 1.42166436e+00\n",
            " 1.41443849e+00 1.38295352e+00 1.33555174e+00 1.31118250e+00\n",
            " 1.30783582e+00 1.28235495e+00 1.23189092e+00 1.20644486e+00\n",
            " 1.18603992e+00 1.16385579e+00 1.16005778e+00 1.13079464e+00\n",
            " 1.12271500e+00 1.09109938e+00 1.08211362e+00 1.06056380e+00\n",
            " 1.04702723e+00 1.03160870e+00 1.00161231e+00 9.78074789e-01\n",
            " 9.64081168e-01 9.35128570e-01 9.25278664e-01 9.21683133e-01\n",
            " 9.16700304e-01 8.89381051e-01 8.67143631e-01 8.59623551e-01\n",
            " 8.57909679e-01 8.49502921e-01 8.34651053e-01 8.20953369e-01\n",
            " 8.10497642e-01 7.97621191e-01 7.75514662e-01 7.70218611e-01\n",
            " 7.63155460e-01 7.60258555e-01 7.30524004e-01 7.20310807e-01\n",
            " 6.95958257e-01 6.93415463e-01 6.88415825e-01 6.80995762e-01\n",
            " 6.77975059e-01 6.66936219e-01 6.45269096e-01 6.37844145e-01\n",
            " 6.36991501e-01 6.33228540e-01 6.20882392e-01 6.08552992e-01\n",
            " 5.98906636e-01 5.85717022e-01 5.73354006e-01 5.68092346e-01\n",
            " 5.63183069e-01 5.60056508e-01 5.45570612e-01 5.42007148e-01\n",
            " 5.40852845e-01 5.32387733e-01 5.29069126e-01 5.19067883e-01\n",
            " 5.13771415e-01 5.08377969e-01 5.01094520e-01 4.92952406e-01\n",
            " 4.90552306e-01 4.85977918e-01 4.83068228e-01 4.76675868e-01\n",
            " 4.73939210e-01 4.64005381e-01 4.61453259e-01 4.54171956e-01\n",
            " 4.49916810e-01 4.45826083e-01 4.41330999e-01 4.39458221e-01\n",
            " 4.34322506e-01 4.29571152e-01 4.23598349e-01 4.15301770e-01\n",
            " 4.14426655e-01 4.10270900e-01 4.04522687e-01 4.02786314e-01\n",
            " 3.97673577e-01 3.87668014e-01 3.86064678e-01 3.80451113e-01\n",
            " 3.75456661e-01 3.72611344e-01 3.66356581e-01 3.63142848e-01\n",
            " 3.59395891e-01 3.58664870e-01 3.56268048e-01 3.51184964e-01\n",
            " 3.48746270e-01 3.43307912e-01 3.42391461e-01 3.40885133e-01\n",
            " 3.37186217e-01 3.32584471e-01 3.30601186e-01 3.25826526e-01\n",
            " 3.23243380e-01 3.19459319e-01 3.18491489e-01 3.15024197e-01\n",
            " 3.12679201e-01 3.08792353e-01 3.06165308e-01 3.05203259e-01\n",
            " 3.00362766e-01 2.98246413e-01 2.96518713e-01 2.92055398e-01\n",
            " 2.90902495e-01 2.89356858e-01 2.87967116e-01 2.82819241e-01\n",
            " 2.79802471e-01 2.79070467e-01 2.75509059e-01 2.72567779e-01\n",
            " 2.69976646e-01 2.69262880e-01 2.66786098e-01 2.64481872e-01\n",
            " 2.63406962e-01 2.59023666e-01 2.58338779e-01 2.53996879e-01\n",
            " 2.52881736e-01 2.50915587e-01 2.50334114e-01 2.48102486e-01\n",
            " 2.45875999e-01 2.44003698e-01 2.43225247e-01 2.41114646e-01\n",
            " 2.39292279e-01 2.37398028e-01 2.33588636e-01 2.31918126e-01\n",
            " 2.30666861e-01 2.30065808e-01 2.27879465e-01 2.24884138e-01\n",
            " 2.23822132e-01 2.22718298e-01 2.18021482e-01 2.17135906e-01\n",
            " 2.16297969e-01 2.15343446e-01 2.14987412e-01 2.12660551e-01\n",
            " 2.11630508e-01 2.08748013e-01 2.07824305e-01 2.06239939e-01\n",
            " 2.04682842e-01 2.03031704e-01 2.00492382e-01 1.99477881e-01\n",
            " 1.98467225e-01 1.95774674e-01 1.93348795e-01 1.92218423e-01\n",
            " 1.91949859e-01 1.91111028e-01 1.89437762e-01 1.88838169e-01\n",
            " 1.86634779e-01 1.85847417e-01 1.83985993e-01 1.82624087e-01\n",
            " 1.82054535e-01 1.80521250e-01 1.78042576e-01 1.77337036e-01\n",
            " 1.76264793e-01 1.74951077e-01 1.74423352e-01 1.73224211e-01\n",
            " 1.70892388e-01 1.70771226e-01 1.69484422e-01 1.67258516e-01\n",
            " 1.66203201e-01 1.65800229e-01 1.64746925e-01 1.63107395e-01\n",
            " 1.61774516e-01 1.61623597e-01 1.58559993e-01 1.57490134e-01\n",
            " 1.57059759e-01 1.55867323e-01 1.54912278e-01 1.53534263e-01\n",
            " 1.53099611e-01 1.51444927e-01 1.49927601e-01 1.49142280e-01\n",
            " 1.47942185e-01 1.47352412e-01 1.46228656e-01 1.44458652e-01\n",
            " 1.43576801e-01 1.42729148e-01 1.41867369e-01 1.41574517e-01\n",
            " 1.40381008e-01 1.39780700e-01 1.38810068e-01 1.37711436e-01\n",
            " 1.37227878e-01 1.35369956e-01 1.35093600e-01 1.34370625e-01\n",
            " 1.34126842e-01 1.32521167e-01 1.31203428e-01 1.30994037e-01\n",
            " 1.30753204e-01 1.29627496e-01 1.28593802e-01 1.27718866e-01\n",
            " 1.27219632e-01 1.25765026e-01 1.25498369e-01 1.24691494e-01\n",
            " 1.24106683e-01 1.22618876e-01 1.22156084e-01 1.20934054e-01\n",
            " 1.20492555e-01 1.19567752e-01 1.18099406e-01 1.17560245e-01\n",
            " 1.16458640e-01 1.16350926e-01 1.15142778e-01 1.14066802e-01\n",
            " 1.13739297e-01 1.13096289e-01 1.12506323e-01 1.12300508e-01\n",
            " 1.12083755e-01 1.10665515e-01 1.09214634e-01 1.08282857e-01\n",
            " 1.08053833e-01 1.07253879e-01 1.06545009e-01 1.05543703e-01\n",
            " 1.04980275e-01 1.04457736e-01 1.03950821e-01 1.02703653e-01\n",
            " 1.01928107e-01 1.01824030e-01 1.01567738e-01 1.00384042e-01\n",
            " 1.00139871e-01 9.89845246e-02 9.88387093e-02 9.76860523e-02\n",
            " 9.74119231e-02 9.71942320e-02 9.70360264e-02 9.59628448e-02\n",
            " 9.55627412e-02 9.44849476e-02 9.42966118e-02 9.37219188e-02\n",
            " 9.30025205e-02 9.19508189e-02 9.17093977e-02 9.11194533e-02\n",
            " 9.08764005e-02 9.03196111e-02 8.98420811e-02 8.91111270e-02\n",
            " 8.89588743e-02 8.81654695e-02 8.70646834e-02 8.67594630e-02\n",
            " 8.63437578e-02 8.60815048e-02 8.50342736e-02 8.46493468e-02\n",
            " 8.39315057e-02 8.33974779e-02 8.30950588e-02 8.30582902e-02\n",
            " 8.16307589e-02 8.12305734e-02 8.11694115e-02 8.00648257e-02\n",
            " 7.96580240e-02 7.96150416e-02 7.87775144e-02 7.79817179e-02\n",
            " 7.78190792e-02 7.71148503e-02 7.67459497e-02 7.63452128e-02\n",
            " 7.56857693e-02 7.53675401e-02 7.51327500e-02 7.47632608e-02\n",
            " 7.41176531e-02 7.36452714e-02 7.30942115e-02 7.26729929e-02\n",
            " 7.22708106e-02 7.16891214e-02 7.13563189e-02 7.09547475e-02\n",
            " 7.08514601e-02 7.05087259e-02 7.01360255e-02 6.96277991e-02\n",
            " 6.90691322e-02 6.86423481e-02 6.79835677e-02 6.76922500e-02\n",
            " 6.72112480e-02 6.67857751e-02 6.60438612e-02 6.58429191e-02\n",
            " 6.54017255e-02 6.51327595e-02 6.48884252e-02 6.45878315e-02\n",
            " 6.44065663e-02 6.37682900e-02 6.35849759e-02 6.34414628e-02\n",
            " 6.30819872e-02 6.20040745e-02 6.16859347e-02 6.15031607e-02\n",
            " 6.13585301e-02 6.06305934e-02 6.01527505e-02 6.01025634e-02\n",
            " 5.97013459e-02 5.94236217e-02 5.93526401e-02 5.88108264e-02\n",
            " 5.84946983e-02 5.79952002e-02 5.77706546e-02 5.75055853e-02\n",
            " 5.68741038e-02 5.64815700e-02 5.63583374e-02 5.60378395e-02\n",
            " 5.54952323e-02 5.52412942e-02 5.49058579e-02 5.44973686e-02\n",
            " 5.43199219e-02 5.41043915e-02 5.35660274e-02 5.32241911e-02\n",
            " 5.27432635e-02 5.25818616e-02 5.24482466e-02 5.19947819e-02\n",
            " 5.18565476e-02 5.15261367e-02 5.08546606e-02 5.07881790e-02\n",
            " 5.06037213e-02 5.02439290e-02 5.01201935e-02 4.97662835e-02\n",
            " 4.95618917e-02 4.89796177e-02 4.89020348e-02 4.88065965e-02\n",
            " 4.84062247e-02 4.80819009e-02 4.76348661e-02 4.75364327e-02\n",
            " 4.75135557e-02 4.69687767e-02 4.65296097e-02 4.61728759e-02\n",
            " 4.58875783e-02 4.54253703e-02 4.52882685e-02 4.48792689e-02\n",
            " 4.46965098e-02 4.44435328e-02 4.43689115e-02 4.38417494e-02\n",
            " 4.37898822e-02 4.34924178e-02 4.32566255e-02 4.29001004e-02\n",
            " 4.28687595e-02 4.25165221e-02 4.21795398e-02 4.18520942e-02\n",
            " 4.17763554e-02 4.14811634e-02 4.12997156e-02 4.10766825e-02\n",
            " 4.07496579e-02 4.03771736e-02 4.02805284e-02 4.01584171e-02\n",
            " 3.96697149e-02 3.91978696e-02 3.91414389e-02 3.89945395e-02\n",
            " 3.87809277e-02 3.85753773e-02 3.83907445e-02 3.81768011e-02\n",
            " 3.78572494e-02 3.77467871e-02 3.75352204e-02 3.72845531e-02\n",
            " 3.70967202e-02 3.70079391e-02 3.63960229e-02 3.63120027e-02\n",
            " 3.60214375e-02 3.59384380e-02 3.58817875e-02 3.55263203e-02\n",
            " 3.52478549e-02 3.50253545e-02 3.48792262e-02 3.47658098e-02\n",
            " 3.44844423e-02 3.40673141e-02 3.39317955e-02 3.39030139e-02\n",
            " 3.35108936e-02 3.34254354e-02 3.33010331e-02 3.29411477e-02\n",
            " 3.27667035e-02 3.24833356e-02 3.22994739e-02 3.20919156e-02\n",
            " 3.19608971e-02 3.18294950e-02 3.15830521e-02 3.14888246e-02\n",
            " 3.11593395e-02 3.09822988e-02 3.08897812e-02 3.05559728e-02\n",
            " 3.04458644e-02 3.02239619e-02 3.01511865e-02 2.98764035e-02\n",
            " 2.97047701e-02 2.94596907e-02 2.92140562e-02 2.90915575e-02\n",
            " 2.87524313e-02 2.86555421e-02 2.84226835e-02 2.83626467e-02\n",
            " 2.81120110e-02 2.80884933e-02 2.78185047e-02 2.77881678e-02\n",
            " 2.74957474e-02 2.72581391e-02 2.70445999e-02 2.66323388e-02\n",
            " 2.65715402e-02 2.64400598e-02 2.63894722e-02 2.63060890e-02\n",
            " 2.59210430e-02 2.57687345e-02 2.57116575e-02 2.55880803e-02\n",
            " 2.54110452e-02 2.53628995e-02 2.50366069e-02 2.48776563e-02\n",
            " 2.47549247e-02 2.44604424e-02 2.42928620e-02 2.41791420e-02\n",
            " 2.40794774e-02 2.39705537e-02 2.38093734e-02 2.35079341e-02\n",
            " 2.34046672e-02 2.32046656e-02 2.30499581e-02 2.27389559e-02\n",
            " 2.27053016e-02 2.24698242e-02 2.23157518e-02 2.21045446e-02\n",
            " 2.20239349e-02 2.19027083e-02 2.17316113e-02 2.15609204e-02\n",
            " 2.14562099e-02 2.13268362e-02 2.12094244e-02 2.10461803e-02\n",
            " 2.09505204e-02 2.07465608e-02 2.05981601e-02 2.03943271e-02\n",
            " 2.02623624e-02 2.00827830e-02 1.99661162e-02 1.99028123e-02\n",
            " 1.97612233e-02 1.95478536e-02 1.93717536e-02 1.93262007e-02\n",
            " 1.91758182e-02 1.90765355e-02 1.90320294e-02 1.88086741e-02\n",
            " 1.86989922e-02 1.84646472e-02 1.82042811e-02 1.81192979e-02\n",
            " 1.80162340e-02 1.79401040e-02 1.78400241e-02 1.77366920e-02\n",
            " 1.75883584e-02 1.74392499e-02 1.72667131e-02 1.71485376e-02\n",
            " 1.70824993e-02 1.69527363e-02 1.69182010e-02 1.66887362e-02\n",
            " 1.65879223e-02 1.65290572e-02 1.63409542e-02 1.62742641e-02\n",
            " 1.61746759e-02 1.58745144e-02 1.58716962e-02 1.57429539e-02\n",
            " 1.56520493e-02 1.55132655e-02 1.53809506e-02 1.52012808e-02\n",
            " 1.51249431e-02 1.50526483e-02 1.49285169e-02 1.47635993e-02\n",
            " 1.45050101e-02 1.44317625e-02 1.44021474e-02 1.42429667e-02\n",
            " 1.41196726e-02 1.39883189e-02 1.38671668e-02 1.38030779e-02\n",
            " 1.35840494e-02 1.34963710e-02 1.34282904e-02 1.33919846e-02\n",
            " 1.32177919e-02 1.31518031e-02 1.30836945e-02 1.29851466e-02\n",
            " 1.27784787e-02 1.26694087e-02 1.25712687e-02 1.24923345e-02\n",
            " 1.24409962e-02 1.24020129e-02 1.23610664e-02 1.22101652e-02\n",
            " 1.19309751e-02 1.18330484e-02 1.17882034e-02 1.16855195e-02\n",
            " 1.16190901e-02 1.14443740e-02 1.12869190e-02 1.12723857e-02\n",
            " 1.12181176e-02 1.11980457e-02 1.10806739e-02 1.08593637e-02\n",
            " 1.07961837e-02 1.07458653e-02 1.06484210e-02 1.06133232e-02\n",
            " 1.04670972e-02 1.04252519e-02 1.02877012e-02 1.02609834e-02\n",
            " 1.00600151e-02 1.00141969e-02 9.91499051e-03 9.79224127e-03\n",
            " 9.71919671e-03 9.67946090e-03 9.44679696e-03 9.41668265e-03\n",
            " 9.34211817e-03 9.26163513e-03 9.11579654e-03 9.04015917e-03\n",
            " 9.00914893e-03 8.91992915e-03 8.89852922e-03 8.74458626e-03\n",
            " 8.64725094e-03 8.55674781e-03 8.46668053e-03 8.35075509e-03\n",
            " 8.26311205e-03 8.20485130e-03 8.06482974e-03 8.02712049e-03\n",
            " 7.97236897e-03 7.86791462e-03 7.76749384e-03 7.66770868e-03\n",
            " 7.61827175e-03 7.49651901e-03 7.40499934e-03 7.36029912e-03\n",
            " 7.25393184e-03 7.18572596e-03 7.15232128e-03 7.10839545e-03\n",
            " 7.02032819e-03 6.98205922e-03 6.82866853e-03 6.72675995e-03\n",
            " 6.69669313e-03 6.57183910e-03 6.49537193e-03 6.37496449e-03\n",
            " 6.33884873e-03 6.29718136e-03 6.19146228e-03 6.17171079e-03\n",
            " 6.06022635e-03 6.03066292e-03 5.89003926e-03 5.82191208e-03\n",
            " 5.71336225e-03 5.70218451e-03 5.57128061e-03 5.49798924e-03\n",
            " 5.48304757e-03 5.45278378e-03 5.37066022e-03 5.32228174e-03\n",
            " 5.18933870e-03 5.09023294e-03 4.97155543e-03 4.91991034e-03\n",
            " 4.76502441e-03 4.68812976e-03 4.57875198e-03 4.53925086e-03\n",
            " 4.48645325e-03 4.32376377e-03 4.22000000e-03 4.19106800e-03\n",
            " 4.05083969e-03 3.92591348e-03 3.89581546e-03 3.81880347e-03\n",
            " 3.76713369e-03 3.61089804e-03 3.54105467e-03 3.49489856e-03\n",
            " 3.39265377e-03 3.28298984e-03 3.25969420e-03 3.10861296e-03\n",
            " 3.06750345e-03 2.91436026e-03 2.72549782e-03 2.70118471e-03\n",
            " 2.64800084e-03 2.61222781e-03 2.45708833e-03 2.41472316e-03\n",
            " 2.36123032e-03 2.27566017e-03 2.20743241e-03 2.14287848e-03\n",
            " 2.06430326e-03 1.90676691e-03 1.77177438e-03 1.69819465e-03\n",
            " 1.69357774e-03 1.56622368e-03 1.44622917e-03 1.21531030e-03\n",
            " 9.68119071e-04 8.08597251e-04 1.05783954e-04 6.33808284e-11]\n",
            "significant_components [ True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False]\n",
            "diver prod 3.995798865816215e-21\n",
            "relevancy prod 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fmnist_images.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iyQjrdVvVBO",
        "outputId": "ff8ae17c-f610-4801-b4e3-af1031763b61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}